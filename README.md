# Asistente Conversacional con RAG, LangGraph y Gemini 2.5 Flash

Este proyecto implementa un asistente conversacional especializado que combina tÃ©cnicas de RecuperaciÃ³n de InformaciÃ³n (RAG), arquitectura modular con **LangGraph**, el modelo **Gemini Flash 2.5** de Google Vertex AI, y trazabilidad completa con **Langfuse**. AdemÃ¡s, incluye una API desarrollada en **FastAPI** y una interfaz de conversaciÃ³n construida en **Streamlit**.

---

## ğŸ“ Estructura del Proyecto

```
TOPICOS_AVANZADOS/
â”œâ”€â”€ agent/                  # Componentes del grafo LangGraph
â”‚   â”œâ”€â”€ ai_graph_state.py
â”‚   â”œâ”€â”€ graph.py
â”‚   â”œâ”€â”€ invoker.py
â”‚   â””â”€â”€ nodes.py
â”œâ”€â”€ app/                    # Backend FastAPI + Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ prompts/                # Archivos .txt utilizados como system prompts
â”‚   â””â”€â”€ prompt_generate.txt
â”œâ”€â”€ utils/                  # Utilidades como Vertex AI y OpenSearch config
â”‚   â””â”€â”€ chat_utils.py
â”œâ”€â”€ notebooks/              # Scripts exploratorios y de embeddings
â”‚   â”œâ”€â”€ embedding.ipynb
â”‚   â””â”€â”€ probando.ipynb
â”œâ”€â”€ app.py                  # AplicaciÃ³n Streamlit (interfaz de usuario)
â”œâ”€â”€ docker-compose.yaml     # OrquestaciÃ³n de FastAPI + OpenSearch
â”œâ”€â”€ bulk_embeddings.json    # Archivo auxiliar de embeddings (carga masiva)
â”œâ”€â”€ base_conocimiento_colombia.xlsx
â””â”€â”€ README.md
```

---

## ğŸ§  Componentes Principales

### 1. **LangGraph con Gemini 2.5 Flash**
- Flujo modular con 2 nodos:
  - `search_node`: realiza bÃºsqueda semÃ¡ntica en OpenSearch usando embeddings de Vertex AI.
  - `generate_node`: genera respuestas contextualizadas usando Gemini Flash 2.5.
- El grafo se compila con `graph.py` y se ejecuta desde `invoker.py`.

### 2. **OpenSearch como fuente documental**

El asistente utiliza OpenSearch como motor de bÃºsqueda para implementar RAG (Retrieval-Augmented Generation). Los documentos se cargan con embeddings vectoriales y se consultan mediante bÃºsquedas semÃ¡nticas (`knn_vector`). A continuaciÃ³n se describe el proceso completo para preparar el entorno:

#### ğŸ§± Requisitos previos de sistema

Antes de desplegar OpenSearch, asegÃºrate de aumentar el lÃ­mite de memoria de mapeo:

```bash
sudo sysctl -w vm.max_map_count=262144
```

#### ğŸš€ Despliegue del entorno (FastAPI + OpenSearch + Dashboards)

```bash
docker-compose up -d --build
```

Este comando levanta tres servicios:
- `opensearch` en `https://localhost:9200`
- `opensearch-dashboards` en `http://localhost:5601`
- `ia-assistant` en `http://localhost:8000`

> ğŸ”’ AsegÃºrate de que los nombres de host en `chat_utils.py` usen `"opensearch"` (no `localhost`) al estar en contenedor.

#### ğŸ—ï¸ CreaciÃ³n del Ã­ndice para embeddings

Una vez levantado OpenSearch, se debe crear el Ã­ndice `topicosindex` con soporte para bÃºsqueda vectorial (KNN). Ejecuta el siguiente comando:

```bash
curl -X PUT "https://localhost:9200/topicosindex" \
  -k \
  -u admin:Nagato123! \
  -H 'Content-Type: application/json' \
  -d '{
    "settings": {
      "index": {
        "knn": true,
        "number_of_shards": 1,
        "number_of_replicas": 0
      }
    },
    "mappings": {
      "properties": {
        "embedding": {
          "type": "knn_vector",
          "dimension": 768
        },
        "metadata": {
          "type": "object",
          "properties": {
            "filename": {
              "type": "keyword"
            },
            "tags": {
              "type": "keyword"
            },
            "id_documento": {
              "type": "integer"
            },
            "keyword": {
              "type": "keyword"
            },
            "title": {
              "type": "text",
              "fields": {
                "keyword": {
                  "type": "keyword",
                  "ignore_above": 256
                }
              }
            }
          }
        },
        "text": {
          "type": "text"
        }
      }
    }
  }'
```

#### ğŸ§  GeneraciÃ³n del archivo con embeddings (`bulk_embeddings.json`)

1. DirÃ­gete a la carpeta `notebooks/` del proyecto.
2. Abre y ejecuta el archivo `embedding.ipynb`.
3. El notebook toma informaciÃ³n estructurada (por ejemplo, desde `base_conocimiento_colombia.xlsx`) y genera un archivo llamado `bulk_embeddings.json`, compatible con el formato de carga masiva de OpenSearch.

> ğŸ“ Actualmente, este procesamiento aplica a datos tabulares. Se puede extender para formatos adicionales como `.txt`, `.pdf`, `.docx`, etc.

#### ğŸ“¥ Carga masiva de documentos en OpenSearch

Una vez generado el archivo `bulk_embeddings.json`, puedes cargarlo directamente al Ã­ndice creado:

```bash
curl -X POST "https://localhost:9200/topicosindex/_bulk" \
  -u admin:Nagato123! \
  -H "Content-Type: application/json" \
  --data-binary @embedding_manual.json \
  -k
```

---

### 3. **FastAPI para servir el asistente**
- Expuesto en el puerto `8000`.
- Endpoint POST `/ask` que recibe una pregunta y retorna la respuesta generada.
- Historial conversacional no se gestiona aquÃ­ (lo maneja Streamlit).

### 4. **Streamlit como interfaz de usuario**
- TÃ­tulo fijo: `Chatea con nuestro asesor interno`.
- Permite enviar mensajes y visualizar el historial.
- BotÃ³n `ğŸ—‘ï¸ Borrar conversaciÃ³n` que reinicia el historial e ID de sesiÃ³n.
- Input de texto siempre en la parte inferior.
- Mensajes del usuario alineados a la derecha.
- Mensajes de la IA alineados a la izquierda.
- El historial se gestiona dentro del frontend y se mantiene por sesiÃ³n.

### 5. **Langfuse para trazabilidad**
- Cada interacciÃ³n con el modelo queda registrada.
- Se integrÃ³ mediante `LangfuseCallbackHandler` en `invoker.py`.
- Las claves estÃ¡n quemadas directamente en el cÃ³digo por simplicidad.

---

## ğŸš€ Instrucciones de Uso

### 1. Posicionarte en el directorio raÃ­z del proyecto

```bash
cd TOPICOS_AVANZADOS
```

### 2. Construir y levantar los servicios

```bash
docker-compose up --build
```

---

### 3. Ejecutar la interfaz en Streamlit

```bash
streamlit run app.py
```

---

## ğŸ§ª Endpoint disponible

**POST /ask**

- **Input JSON:**
```json
{ "question": "Â¿QuÃ© es el cambio climÃ¡tico?" }
```

- **Output JSON:**
```json
{ "response": "El cambio climÃ¡tico es..." }
```

---

## ğŸ“ Notas finales

- Todos los prompts estÃ¡n cargados desde archivos `.txt` en `prompts/`, lo cual permite su modificaciÃ³n en tiempo real durante desarrollo.
- Las claves de Langfuse estÃ¡n quemadas directamente en `invoker.py`.
- La gestiÃ³n del historial y la experiencia conversacional completa se maneja desde el frontend en Streamlit.

---

## ğŸ› ï¸ Requisitos tÃ©cnicos

- Docker y Docker Compose
- Python 3.10+
- Acceso habilitado a Vertex AI y al modelo `gemini-2.5-flash-preview-04-17`
- OpenSearch correctamente configurado localmente

---

## Exponer ngrok 

```bash
ngrok http --url=correct-bengal-whole.ngrok-free.app 8501
```

