# Proyecto T√≥picos avanzados en IA

Este proyecto implementa un **asistente de inteligencia artificial** cuyo prop√≥sito es facilitar a los estudiantes de la **Pontificia Universidad Javeriana** la consulta √°gil y precisa del **Reglamento Estudiantil**.  

La soluci√≥n se dise√±√≥ y despleg√≥ **√≠ntegramente en Google Cloud Platform (GCP)**, aprovechando los servicios incluidos en la capa gratuita. Para habilitar respuestas contextuales, se construy√≥ una **base de conocimiento** donde **cada p√°gina del reglamento** se trata como un documento independiente. Esa granularidad permite aplicar la t√©cnica de **Retrieval-Augmented Generation (RAG)**, garantizando que el asistente recupere los fragmentos relevantes del reglamento y genere respuestas fundamentadas en el texto oficial. Adicionalmente, para el desarrollo del proyecto se utilizo la siguiente arquitectura:


<div align="center">
  <img src="images/arquitectura_poc.png" alt="streamlit" width="700"/>
</div>


| # | Componente | Descripci√≥n |
|---|------------|-------------|
| 1 | **Generaci√≥n de embeddings** | 1. El PDF del Reglamento se **divide por p√°ginas** y cada p√°gina se transforma a imagen.<br>2. Sobre cada imagen se ejecuta **Gemini 2.5 Flash (multimodal)** para extraer el texto con alta fidelidad.<br>3. El texto se normaliza y se genera la representaci√≥n vectorial con el modelo **`text-multilingual-embedding-002`** de Vertex AI (768 dim).<br>4. Todo el proceso se orquesta desde un **Notebook** alojado en la misma VM y al final se produce un archivo `bulk_embeddings.json`. |
| 2 | **Base de conocimiento (OpenSearch)** | - Se despliega **OpenSearch** en un contenedor dentro de la VM.<br>- Se crea el √≠ndice `topicosindex` con soporte **KNN** y dimensi√≥n **768**.<br>- El archivo `bulk_embeddings.json` se carga v√≠a API `_bulk`, quedando cada p√°gina del reglamento como un documento con su vector. |
| 3 | **Asistente (LangChain + LangGraph + FastAPI)** | - Implementado con **LangGraph**: dos nodos.<br>  ‚Ä¢ `search_node` ‚Üí recupera los `k` documentos m√°s relevantes desde OpenSearch.<br>  ‚Ä¢ `generate_node` ‚Üí construye el mensaje de sistema y llama a **Gemini 2.5 Pro** para redactar la respuesta.<br>- Se eligi√≥ Gemini 2.5 Pro porque est√° en *preview* (sin coste) y ofrece mayor capacidad de razonamiento.<br>- El grafo se expone a trav√©s de un **backend FastAPI** (endpoint `POST /ask`).<br>- Todo el stack (FastAPI, OpenSearch y Dashboards) se **orquesta mediante `docker-compose.yaml`**, facilitando la puesta en marcha con un solo comando. |
| 4 | **Frontend (Streamlit + ngrok)** | - Streamlit expone una interfaz chat que:<br>  ‚Ä¢ Mantiene el historial de la conversaci√≥n.<br>  ‚Ä¢ Permite reiniciar sesi√≥n.<br>- Para compartir la app p√∫blicamente sin exponer puertos se usa **ngrok**, que crea un t√∫nel HTTPS temporal hacia la instancia de Streamlit. |
| 5 | **Observabilidad (Langfuse)** | - **Langfuse** registra cada conversaci√≥n de principio a fin bajo un mismo **ID de sesi√≥n**, de forma que todas las preguntas y respuestas quedan agrupadas.<br>- En su panel se pueden ver m√©tricas como tiempo de respuesta, uso de tokens y los pasos internos que sigui√≥ el asistente, lo que facilita el seguimiento y la mejora continua. |

> **Todo el stack corre en una √∫nica VM de Compute Engine** (entorno de desarrollo/experimentaci√≥n) aprovechando la capa gratuita de Google Cloud Platform.




<div align="center">
  <img src="images/arquitectura_prod.png" alt="streamlit" width="700"/>
</div>





















# Asistente Conversacional con RAG, LangGraph y Gemini 2.5 Flash

Este proyecto implementa un asistente conversacional especializado que combina t√©cnicas de Recuperaci√≥n de Informaci√≥n (RAG), arquitectura modular con **LangGraph**, el modelo **Gemini Flash 2.5** de Google Vertex AI, y trazabilidad completa con **Langfuse**. Adem√°s, incluye una API desarrollada en **FastAPI** y una interfaz de conversaci√≥n construida en **Streamlit**.

---

## üìÅ Estructura del Proyecto

```
TOPICOS_AVANZADOS/
‚îú‚îÄ‚îÄ agent/                  # Componentes del grafo LangGraph
‚îÇ   ‚îú‚îÄ‚îÄ ai_graph_state.py
‚îÇ   ‚îú‚îÄ‚îÄ graph.py
‚îÇ   ‚îú‚îÄ‚îÄ invoker.py
‚îÇ   ‚îî‚îÄ‚îÄ nodes.py
‚îú‚îÄ‚îÄ app/                    # Backend FastAPI + Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ prompts/                # Archivos .txt utilizados como system prompts
‚îÇ   ‚îî‚îÄ‚îÄ prompt_generate.txt
‚îú‚îÄ‚îÄ utils/                  # Utilidades como Vertex AI y OpenSearch config
‚îÇ   ‚îî‚îÄ‚îÄ chat_utils.py
‚îú‚îÄ‚îÄ notebooks/              # Scripts exploratorios y de embeddings
‚îÇ   ‚îú‚îÄ‚îÄ embedding.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ probando.ipynb
‚îú‚îÄ‚îÄ app.py                  # Aplicaci√≥n Streamlit (interfaz de usuario)
‚îú‚îÄ‚îÄ docker-compose.yaml     # Orquestaci√≥n de FastAPI + OpenSearch
‚îú‚îÄ‚îÄ bulk_embeddings.json    # Archivo auxiliar de embeddings (carga masiva)
‚îú‚îÄ‚îÄ base_conocimiento_colombia.xlsx
‚îî‚îÄ‚îÄ README.md
```

---

## üß† Componentes Principales

### 1. **LangGraph con Gemini 2.5 Flash**
- Flujo modular con 2 nodos:
  - `search_node`: realiza b√∫squeda sem√°ntica en OpenSearch usando embeddings de Vertex AI.
  - `generate_node`: genera respuestas contextualizadas usando Gemini Flash 2.5.
- El grafo se compila con `graph.py` y se ejecuta desde `invoker.py`.

### 2. **OpenSearch como fuente documental**

El asistente utiliza OpenSearch como motor de b√∫squeda para implementar RAG (Retrieval-Augmented Generation). Los documentos se cargan con embeddings vectoriales y se consultan mediante b√∫squedas sem√°nticas (`knn_vector`). A continuaci√≥n se describe el proceso completo para preparar el entorno:

#### üß± Requisitos previos de sistema

Antes de desplegar OpenSearch, aseg√∫rate de aumentar el l√≠mite de memoria de mapeo:

```bash
sudo sysctl -w vm.max_map_count=262144
```

#### üöÄ Despliegue del entorno (FastAPI + OpenSearch + Dashboards)

```bash
docker-compose up -d --build
```

Este comando levanta tres servicios:
- `opensearch` en `https://localhost:9200`
- `opensearch-dashboards` en `http://localhost:5601`
- `ia-assistant` en `http://localhost:8000`

> üîí Aseg√∫rate de que los nombres de host en `chat_utils.py` usen `"opensearch"` (no `localhost`) al estar en contenedor.

#### üèóÔ∏è Creaci√≥n del √≠ndice para embeddings

Una vez levantado OpenSearch, se debe crear el √≠ndice `topicosindex` con soporte para b√∫squeda vectorial (KNN). Ejecuta el siguiente comando:

```bash
curl -X PUT "https://localhost:9200/topicosindex" \
  -k \
  -u admin:Nagato123! \
  -H 'Content-Type: application/json' \
  -d '{
    "settings": {
      "index": {
        "knn": true,
        "number_of_shards": 1,
        "number_of_replicas": 0
      }
    },
    "mappings": {
      "properties": {
        "embedding": {
          "type": "knn_vector",
          "dimension": 768
        },
        "metadata": {
          "type": "object",
          "properties": {
            "filename": {
              "type": "keyword"
            },
            "tags": {
              "type": "keyword"
            },
            "id_documento": {
              "type": "integer"
            },
            "keyword": {
              "type": "keyword"
            },
            "title": {
              "type": "text",
              "fields": {
                "keyword": {
                  "type": "keyword",
                  "ignore_above": 256
                }
              }
            }
          }
        },
        "text": {
          "type": "text"
        }
      }
    }
  }'
```

#### üß† Generaci√≥n del archivo con embeddings (`bulk_embeddings.json`)

1. Dir√≠gete a la carpeta `notebooks/` del proyecto.
2. Abre y ejecuta el archivo `embedding.ipynb`.
3. El notebook toma informaci√≥n estructurada (por ejemplo, desde `base_conocimiento_colombia.xlsx`) y genera un archivo llamado `bulk_embeddings.json`, compatible con el formato de carga masiva de OpenSearch.

> üìù Actualmente, este procesamiento aplica a datos tabulares. Se puede extender para formatos adicionales como `.txt`, `.pdf`, `.docx`, etc.

#### üì• Carga masiva de documentos en OpenSearch

Una vez generado el archivo `bulk_embeddings.json`, puedes cargarlo directamente al √≠ndice creado:

```bash
curl -X POST "https://localhost:9200/topicosindex/_bulk" \
  -u admin:Nagato123! \
  -H "Content-Type: application/json" \
  --data-binary @embedding_manual.json \
  -k
```

---

### 3. **FastAPI para servir el asistente**
- Expuesto en el puerto `8000`.
- Endpoint POST `/ask` que recibe una pregunta y retorna la respuesta generada.
- Historial conversacional no se gestiona aqu√≠ (lo maneja Streamlit).

### 4. **Streamlit como interfaz de usuario**
- T√≠tulo fijo: `Chatea con nuestro asesor interno`.
- Permite enviar mensajes y visualizar el historial.
- Bot√≥n `üóëÔ∏è Borrar conversaci√≥n` que reinicia el historial e ID de sesi√≥n.
- Input de texto siempre en la parte inferior.
- Mensajes del usuario alineados a la derecha.
- Mensajes de la IA alineados a la izquierda.
- El historial se gestiona dentro del frontend y se mantiene por sesi√≥n.

### 5. **Langfuse para trazabilidad**
- Cada interacci√≥n con el modelo queda registrada.
- Se integr√≥ mediante `LangfuseCallbackHandler` en `invoker.py`.
- Las claves est√°n quemadas directamente en el c√≥digo por simplicidad.

---

## üöÄ Instrucciones de Uso

### 1. Posicionarte en el directorio ra√≠z del proyecto

```bash
cd TOPICOS_AVANZADOS
```

### 2. Construir y levantar los servicios

```bash
docker-compose up --build
```

---

### 3. Ejecutar la interfaz en Streamlit

```bash
streamlit run app.py
```

---

## üß™ Endpoint disponible

**POST /ask**

- **Input JSON:**
```json
{ "question": "¬øQu√© es el cambio clim√°tico?" }
```

- **Output JSON:**
```json
{ "response": "El cambio clim√°tico es..." }
```

---

## üìù Notas finales

- Todos los prompts est√°n cargados desde archivos `.txt` en `prompts/`, lo cual permite su modificaci√≥n en tiempo real durante desarrollo.
- Las claves de Langfuse est√°n quemadas directamente en `invoker.py`.
- La gesti√≥n del historial y la experiencia conversacional completa se maneja desde el frontend en Streamlit.

---

## üõ†Ô∏è Requisitos t√©cnicos

- Docker y Docker Compose
- Python 3.10+
- Acceso habilitado a Vertex AI y al modelo `gemini-2.5-flash-preview-04-17`
- OpenSearch correctamente configurado localmente

---

## Exponer ngrok 

```bash
ngrok http --url=correct-bengal-whole.ngrok-free.app 8501
```

